---
title: "Assignment 3 File"
output:
  html_document:
    df_print: paged
---
\vspace{0.25in}

### Due April 28, 2022
### Worth 40 points total

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(text2vec)
library(tm)
library(SnowballC)
library(glmnet)
library(vip)
library(naivebayes)
library(ranger)
library(xgboost)
```

## Problem Overview

"Clickbait" is online content whose main purpose is to attract attention and encourage visitors to click on a link to a particular web page. The dataset for this assignment consists of clickbait titles (drawn from known clickbait websites such as Buzzfeed) and non-clickbait article titles drawn from reputable sources. The goal of this assignment will be to train predictive models to differentiate between clickbait and non-clickbait headlines.

The goal of this assignment is to get hands-on practice with text featurization and advanced predictive modeling techniques, including Ridge, Lasso, ensemble methods, and Naive Bayes.

RUBRIC: There are three possible grades on this assignment: Fail (F), Pass (P), and High Pass (H). If you receive an F then you will have one more chance to turn it in to receive a P. If you receive H on 3 out of the 4 assignments this semester you'll get a bonus point on your final average.

1.  Turn in a well-formatted compiled HTML document using R markdown. If you turn in a different file type or your code doesn't compile, you will be asked to redo the assignment.
2.  Provide clear answers to the questions and the correct R commands as necessary, in the appropriate places. You may answer up to three sub-questions incorrectly and still receive a P on this assignment (for example, 1(a) counts as one sub-question). If you answer all sub-questions correctly on your first submission you will receive an H.
3.  The entire document must be clear, concise, readable, and well-formatted. If your assignment is unreadable or if you include more output than necessary to answer the questions you will be asked to redo the assignment.

Note that this assignment is somewhat open-ended and there are many ways to answer these questions. I don't require that we have exactly the same answers in order for you to receive full credit.


The following code block does some initial setup, including:

1. Reading the dataset (make sure to set your working directory)
2. Creating the target variable
3. Setting the random seed
4. Splitting into 70% training and 30% validation data

```{r data_setup}

cb_data <- read_csv("clickbait_headlines.csv") %>%
  mutate(cb_numeric = clickbait,
    clickbait = as.factor(clickbait))

set.seed(1)
train_rows <- sample(nrow(cb_data),.7*nrow(cb_data))
cb_train <- cb_data[train_rows,]
cb_valid <- cb_data[-train_rows,]

```

## 0: Example answer

What is the base rate (percent of clickbait articles) in the training data?

**ANSWER: 51.26% of the headlines in this dataset are clickbait.**

```{r code0}
counts <- table(cb_train$clickbait)
counts[0]/sum(counts)
```

## 1: Text Featurization

a. Create the clickbait article vocabulary from their titles using the following parameters: lowercase the words, remove numbers and punctuation, remove stopwords, perform stemming. Include both unigrams and bigrams. Prune the resulting vocabulary to only include terms that occur in at least 10 article titles.

**ANSWER TO QUESTION 1a HERE:** 

```{r code_1a}
preprocess = tolower

cleaning_tokenizer <- function(v) {
  v %>%
    removeNumbers %>%  #remove all numbers
    removePunctuation %>%  #remove all punctuation
    removeWords(stopwords(kind="en")) %>%  #remove stopwords
    stemDocument %>%
    word_tokenizer 
}

clean_token = cleaning_tokenizer

iter_train = itoken(cb_train$article_title, 
                  preprocessor = preprocess, 
                  tokenizer = clean_token, 
                  ids = cb_train$article_id, 
                  progressbar = FALSE)

vocabulary = create_vocabulary(iter_train, ngram = c(1L, 2L))

vocabulary = prune_vocabulary(vocabulary, term_count_min = 10)


```

b. Vectorize the training and validation emails and convert them into TFIDF representation.

**ANSWER TO QUESTION 1b HERE: ** 

```{r code_1b}
vectorizer = vocab_vectorizer(vocabulary)

# Convert the training documents into a DTM
# Making it a binary BOW matrix
dtm_train = create_dtm(iter_train, vectorizer)

iter_valid = itoken(cb_valid$article_title, 
                  preprocessor = preprocess, 
                  tokenizer = clean_token, 
                  ids = cb_valid$article_id, 
                  progressbar = FALSE)

dtm_valid = create_dtm(iter_valid, vectorizer)

# Making a TFIDF DTM
tfidf = TfIdf$new()

dtm_train_tfidf = fit_transform(dtm_train, tfidf)
dtm_valid_tfidf = fit_transform(dtm_valid, tfidf)

```


## 2: Ridge and Lasso

a. Train 5-fold cross validated Ridge and lasso with lambda selected from a grid of 100 values ranging between 10^-7 and 10^7 (hint: use cv.glmnet). Include the plots showing the effect of lambda. 

**ANSWER TO QUESTION 2a HERE:  ** 

```{r code 2a}
train_y <- cb_train$clickbait
valid_y <- cb_valid$clickbait

grid <- 10^seq(7,-7,length=100)
k<-5

# Ridge Model
ridge <- cv.glmnet(dtm_train, train_y, family="binomial", alpha=0, lambda=grid, nfolds=k)
plot(ridge)


# Lasso model
lasso <- cv.glmnet(dtm_train, train_y, family="binomial", alpha=1, lambda=grid, nfolds=k)
plot(lasso)

```

b. Do the ridge and lasso models have the same best lambda? Inspect the coefficients of the best lasso and ridge models. Are the coefficients exactly the same? 

**ANSWER TO QUESTION 2b HERE:** 

Coefficients for both models are not same. 
Ridge models have best lambda as 0.017, whereas Lasso models have best lambda as 0.0017

```{r code 2b}
#Getting Minimum Lamda Value for finding best accuracy

bestLamda_ridge <- ridge$lambda.min
predict(ridge,type="coef",s=bestLamda_ridge)
bestLamda_ridge

bestLamda_lasso <- lasso$lambda.min
predict(lasso,type="coef",s=bestLamda_lasso)
bestLamda_lasso
```

c. Using the best lasso and ridge models, make predictions in the validation set. What are the accuracies of your best ridge and lasso models?

**ANSWER TO QUESTION 2c HERE: ** 

Accuracy for Ridge Model: 0.9143
Accuracy for Lasso Model: 0.9146

```{r code 2c}
predict_ridge <- predict(ridge, s=bestLamda_ridge, newx = dtm_valid,type="response")
classify_ridge <- ifelse(predict_ridge > 0.5, 1, 0)
accuracy_ridge = mean(ifelse(classify_ridge == valid_y, 1, 0))
accuracy_ridge

predict_lasso <- predict(lasso, s=bestLamda_lasso, newx = dtm_valid,type="response")
classify_lasso <- ifelse(predict_lasso > 0.5, 1, 0)
accuracy_lasso = mean(ifelse(classify_lasso == valid_y, 1, 0))
accuracy_lasso

```


## 3: Ensemble Methods

a. Use ranger() to train a random forest model with 500 trees and m = 15. (Be patient, this one takes a few minutes to run). Do the predictions/classifications in the validation set and report the accuracy. Create a variable importance plot. Which are the most important terms?

**ANSWER TO QUESTION 3a HERE:** 

Most important terms in order: thing, peopl, will, know, actual, can, time, kill, here, us

The accuracy of Random Forest model is 0.902

```{r code 3a}
# Random Forest
random_forest <- ranger(x = dtm_train, y = train_y,
                 mtry=15, num.trees=500,
                 importance="impurity",
                 probability = TRUE)

random_forest_predicts <- predict(random_forest, data=dtm_valid)$predictions[,2]
random_forest_classifications <- ifelse(random_forest_predicts>0.5, 1, 0)
random_forest_accuracy <- mean(ifelse(random_forest_classifications == valid_y, 1, 0))
random_forest_accuracy

vip(random_forest)

```

b. Use xgboost() to train a boosting model with max.depth = 2, eta = 1, and nrounds = 1000. Do the classifications in the validation set and make predictions. Report the accuracy of your boosting model. Create another variable importance plot. Are the most important terms the same as for the random forest model?


**ANSWER TO QUESTION 3b HERE: ** 

Accuracy of the boosting model: 0.91033

Here we can observe that the most important terms are the same but the order has changed.

```{r code 3b}
train_y_num = cb_train$cb_numeric
valid_y_num = cb_valid$cb_numeric

xgbst <- xgboost(data = dtm_train, label = train_y_num, max.depth = 2, eta = 1, nrounds = 1000,  objective = "binary:logistic")

xgbst_predicts <- predict(xgbst, dtm_valid)
xgbst_classifications <- ifelse(xgbst_predicts > 0.5, 1, 0)
xgbst_accuracy <- mean(ifelse(xgbst_classifications == valid_y_num, 1, 0))
xgbst_accuracy

vip(xgbst,num_features = 20)
```


## 4: Naive Bayes

a. Train two naive bayes models using multinomial_naive_bayes() - one with laplace = 3 and one with laplace = 0. 


**ANSWER TO QUESTION 4a HERE:** 

```{r code_4a}

naive_bayes <- multinomial_naive_bayes(x = dtm_train, y = train_y, laplace = 3)

```

b. For both models, make predictions in the validation set, classify using a cutoff of 0.5, and report the accuracy. Do the two models have different performance?

**ANSWER TO QUESTION 4b HERE: ** 

Accuracy of Naive Bayes model by using Laplace : 0.92233

```{r code_4b}

naive_bayes_predicts <- predict(naive_bayes, dtm_valid, type = "prob")[,2]
naive_bayes_classifications <- ifelse(naive_bayes_predicts > 0.5, 1, 0)
naive_bayes_accuracy <- mean(ifelse(naive_bayes_classifications == valid_y, 1, 0))
naive_bayes_accuracy

```

c. Inspect some of the misclassifications. Find two false positives and two false negatives and explain why you think they may have been misclassified.

**ANSWER TO QUESTION 4c HERE:** 

The model tends to give more weight to superlative adjectives which tend to be negative and this might result in false positive. 
Example for it:
"A Burmese Icon Tends a Flickering Flame"

There could be numericals and stop words in the headlines, so removing them might have changed meaning of the sentence resulting this false negative. 
Examples for it:
"8 WTF Moments Of The 2016 Presidential Race", "14 Of The Hottest Topics From 2015"

```{r code_4c}

false_positive <- cb_valid %>%
  filter(naive_bayes_classifications == 1 & clickbait == 0)

false_negative <- cb_valid %>%
  filter(naive_bayes_classifications == 0 & clickbait == 1)

view(false_positive)
view(false_negative)

```


## 5: Variable Selection

a. Re-tokenize the article titles using the following parameters: DON'T remove numbers, DON'T remove punctuation, DON'T remove stop words, and DON'T stem the document. Create the vocabulary, including up to 4-grams. DON'T prune the vocabulary (yet).

**ANSWER TO QUESTION 5a HERE:** 
```{r code_5a}

preprocess = tolower

cleaning_tokenizer <- function(v) {
  v %>%
    word_tokenizer 
}

clean_token = cleaning_tokenizer

# Iterating over individual documents and converting them to tokens
iter_train = itoken(cb_train$article_title, 
                  preprocessor = preprocess, 
                  tokenizer = clean_token, 
                  ids = cb_train$article_id, 
                  progressbar = FALSE)


iter_valid = itoken(cb_valid$article_title, 
                  preprocessor = preprocess, 
                  tokenizer = clean_token, 
                  ids = cb_valid$article_id, 
                  progressbar = FALSE)

vocabulary = create_vocabulary(iter_train, ngram = c(1L, 4L))
```


b. Using smoothed Naive Bayes, make a plot showing the effect of vocabulary size on validation accuracy. What is the effect of vocabulary size on predictive performance? Does it appear that including "too many terms" will cause the model to overfit?

Hint: try pruning the vocabulary using a range of max_vocab_size values from 10 to the total size of the vocabulary. Create a list of your vocabulary sizes and loop over each size. You will have to re-vectorize the training and validation data before training each a new model. Plot the log of the resulting vocabulary size vs. validation accuracy.

**ANSWER TO QUESTION 5b HERE:** 
Increase in number of terms will led to overfitting the models and as a result give a high performance.

```{r code_5b}
vocabulary_sizes = c(10, 100, 500, 1000, 5000, 10000, 25000, 50000, 75000, 100000, 125000, nrow(vocabulary))
vo_accuracy <- rep(0, length(vocabulary_sizes))

calculate_accuracy <- function(naive_bayes,dtm_valid_new) {
  naive_bayes_predicts <- predict(naive_bayes, new_dtm_valid, type = "prob")[,2]
  naive_bayes_classifications <- ifelse(naive_bayes_predicts > 0.5, 1, 0)
  naive_bayes_accuracy <- mean(ifelse(naive_bayes_classifications == valid_y, 1, 0))
  return(naive_bayes_accuracy)
}

for(i in 1:length(vocabulary_sizes)){
 
  size = as.integer(vocabulary_sizes[i])
  pruned_vocabulary = prune_vocabulary(vocabulary, vocab_term_max = size)
  
  vectorizer_new = vocab_vectorizer(pruned_vocabulary)

  dtm_train_new = create_dtm(iter_train, vectorizer_new)
  dtm_valid_new = create_dtm(iter_valid, vectorizer_new)
  
  naive_bayes <- multinomial_naive_bayes(x = dtm_train_new, y = train_y, laplace = 3)
  naive_bayes_predicts <- predict(naive_bayes, dtm_valid_new, type = "prob")[,2]
  naive_bayes_classifications <- ifelse(naive_bayes_predicts > 0.5, 1, 0)
  naive_bayes_accuracy <- mean(ifelse(naive_bayes_classifications == valid_y, 1, 0))
  vo_accuracy[i] = naive_bayes_accuracy
}

plot(log(vocabulary_sizes), vo_accuracy*100, type = "l", col = "blue", ylim = c(60,100), pch = 20, xlab = "Log of Vocabulary size", ylab = "Accuracy", lwd = 2)
```